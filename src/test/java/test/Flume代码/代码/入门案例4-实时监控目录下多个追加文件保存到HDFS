#1、定义agent、source、channel、sink的名称
a1.sources = r1
a1.channels = c1
a1.sinks = k1
#2、描述source
a1.sources.r1.type = TAILDIR
#指定监控的目录/文件组的名称
a1.sources.r1.filegroups = f1 f2
#指定f1 group组监控的目录/文件
a1.sources.r1.filegroups.f1 = /opt/module/flume/datas/job1/*
#指定f2 group组监控的目录/文件
a1.sources.r1.filegroups.f2 = /opt/module/flume/datas/job2/.*log
#指定positionFile文件的保存路径[positionFile文件是记录上一次采集到了哪个位置]
a1.sources.r1.positionFile = /opt/module/flume/position.json
#定义source每批次采集多少数据
a1.sources.r1.batchSize = 100

#3、描述channel
a1.channels.c1.type = memory
#指定channel的容量
a1.channels.c1.capacity = 1000
#指定事务的容量 <=channel的容量
a1.channels.c1.transactionCapacity = 1000

#4、描述sink
a1.sinks.k1.type = hdfs
#定义hdfs数据的保存路径
a1.sinks.k1.hdfs.path = /flume/events/%Y%m%d
#定义hdfs保存文件的前缀
a1.sinks.k1.hdfs.filePrefix = log-
#指定间隔多久滚动生成新文件
a1.sinks.k1.hdfs.rollInterval = 3600
#指定文件多大之后滚动生成新文件
a1.sinks.k1.hdfs.rollSize = 134210000
#指定文件写入多少个Event后滚动生成新文件
a1.sinks.k1.hdfs.rollCount = 0
#定义sink一个批次拉取多少数据，必须<=事务的容量
a1.sinks.k1.hdfs.batchSize = 100
#指定写入hdfs的时候数据的压缩格式
#a1.sinks.k1.hdfs.codeC = 100
#指定以什么文件格式写入hdfs[SequenceFile-序列化文件, DataStream-文本文件, CompressedStream-压缩文件]
a1.sinks.k1.hdfs.fileType = DataStream
#是否按照指定的时间间隔生成文件夹
a1.sinks.k1.hdfs.round = true
#指定生成文件夹的时间值
a1.sinks.k1.hdfs.roundValue = 24
#指定生成文件夹的时间单位
a1.sinks.k1.hdfs.roundUnit = hour
#是否使用本地的时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#5、关联source->channel->sink
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1